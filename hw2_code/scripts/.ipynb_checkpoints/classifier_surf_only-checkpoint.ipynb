{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "#s_trn_file_path = \"../surf_bow/surf_2000_trn.csv\"\n",
    "#s_val_file_path = \"../surf_bow/surf_2000_val.csv\"\n",
    "#s_test_file_path = \"../surf_bow/surf_2000_test.csv\"\n",
    "\n",
    "s_trn_file_path = \"../cnn_bow/cnn_trn.csv\"\n",
    "s_val_file_path = \"../cnn_bow/cnn_val.csv\"\n",
    "s_test_file_path = \"../cnn_bow/cnn_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836, 512)\n",
      "         C0        C1        C2        C3        C4        C5        C6  \\\n",
      "0  0.010122  0.018024  0.000900  0.010409  0.000605  0.005340  0.008419   \n",
      "1  0.050114  0.013067  0.009877  0.061242  0.038988  0.018305  0.026640   \n",
      "2  0.009906  0.026293  0.034094  0.054672  0.082611  0.010293  0.056078   \n",
      "3  0.013478  0.066380  0.017004  0.015395  0.008548  0.069515  0.017058   \n",
      "4  0.029833  0.095173  0.027000  0.061712  0.041942  0.023729  0.029055   \n",
      "\n",
      "         C7        C8        C9  ...      C502      C503      C504      C505  \\\n",
      "0  0.008305  0.030344  0.050468  ...  0.003989  0.030597  0.087661  0.008131   \n",
      "1  0.027461  0.018837  0.041661  ...  0.018111  0.005752  0.048789  0.015635   \n",
      "2  0.045595  0.024558  0.089477  ...  0.014988  0.028714  0.064525  0.037818   \n",
      "3  0.028123  0.053700  0.022339  ...  0.011754  0.024436  0.040179  0.059960   \n",
      "4  0.016515  0.044979  0.047156  ...  0.007343  0.017801  0.015857  0.028200   \n",
      "\n",
      "       C506      C507      C508      C509      C510      C511  \n",
      "0  0.001509  0.095269  0.042853  0.031456  0.089824  0.005454  \n",
      "1  0.052522  0.030659  0.041267  0.053667  0.005310  0.041180  \n",
      "2  0.000000  0.063956  0.027636  0.040619  0.039285  0.011542  \n",
      "3  0.016336  0.025207  0.056931  0.042980  0.022263  0.035187  \n",
      "4  0.009961  0.060918  0.087240  0.070330  0.006292  0.026669  \n",
      "\n",
      "[5 rows x 512 columns]\n",
      "(836,)\n",
      "0    P001\n",
      "1    P003\n",
      "2    P000\n",
      "3    P000\n",
      "4    P000\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "s_train_df = pd.read_csv(s_trn_file_path, index_col='Unnamed: 0')\n",
    "s_train_df.drop(['name'], axis=1, inplace=True )\n",
    "\n",
    "train_df = s_train_df\n",
    "train_df.target.fillna('P000', inplace=True)\n",
    "train_df.fillna(0.0, inplace=True)\n",
    "\n",
    "##\n",
    "\n",
    "### tf_idf conversion\n",
    "\n",
    "# 1. Save target column, and drop if from dataframe\n",
    "train_df_target = pd.DataFrame(train_df['target'], columns=['target'])\n",
    "train_df.drop(['target'], axis=1, inplace=True )\n",
    "\n",
    "# 2. Replace frequencies with tf_idf scores\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(train_df)\n",
    "X_train_tf = tf_transformer.transform(train_df)\n",
    "train_df = pd.DataFrame(X_train_tf.todense(), columns=train_df.columns.values)\n",
    "\n",
    "# 3. Add back the target column\n",
    "train_df = pd.concat([train_df, train_df_target], axis=1)\n",
    "\n",
    "##\n",
    "\n",
    "s_test_df = pd.read_csv(s_val_file_path, index_col='Unnamed: 0')\n",
    "s_test_df.drop(['name'], axis=1, inplace=True )\n",
    "\n",
    "test_df = s_test_df\n",
    "test_df.target.fillna('P000', inplace=True)\n",
    "test_df.fillna(0.0, inplace=True)\n",
    "\n",
    "##\n",
    "\n",
    "### tf_idf conversion\n",
    "\n",
    "# 1. Save target column, and drop if from dataframe\n",
    "test_df_target = pd.DataFrame(test_df['target'], columns=['target'])\n",
    "test_df.drop(['target'], axis=1, inplace=True )\n",
    "\n",
    "# 2. Replace frequencies with tf_idf scores\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(test_df)\n",
    "X_train_tf = tf_transformer.transform(test_df)\n",
    "test_df = pd.DataFrame(X_train_tf.todense(), columns=test_df.columns.values)\n",
    "\n",
    "# 3. Add back the target column\n",
    "test_df = pd.concat([test_df, test_df_target], axis=1)\n",
    "\n",
    "##\n",
    "\n",
    "# Machine Learning\n",
    "prediction_var = list(train_df.columns)\n",
    "prediction_var.remove('target')\n",
    "\n",
    "# Get input training data\n",
    "train_X = train_df[prediction_var]\n",
    "\n",
    "# Get input target variable\n",
    "train_y = train_df.target\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_X.head())\n",
    "print(train_y.shape)\n",
    "print(train_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 512)\n",
      "         C0        C1        C2        C3        C4        C5        C6  \\\n",
      "0  0.002256  0.014969  0.068157  0.030224  0.043937  0.050992  0.023642   \n",
      "1  0.026249  0.055684  0.013937  0.044967  0.015883  0.012727  0.025819   \n",
      "2  0.038058  0.105269  0.053092  0.048149  0.050134  0.022210  0.017858   \n",
      "3  0.014148  0.046026  0.011328  0.023957  0.015194  0.031181  0.017096   \n",
      "4  0.016313  0.020703  0.003254  0.023225  0.019445  0.044612  0.015210   \n",
      "\n",
      "         C7        C8        C9  ...      C502      C503      C504      C505  \\\n",
      "0  0.008669  0.020163  0.027555  ...  0.002847  0.008360  0.019811  0.038092   \n",
      "1  0.029802  0.018709  0.104177  ...  0.013205  0.017230  0.020660  0.015680   \n",
      "2  0.016442  0.024401  0.035477  ...  0.036244  0.017640  0.019987  0.040829   \n",
      "3  0.045868  0.023756  0.017671  ...  0.005807  0.039719  0.048694  0.012453   \n",
      "4  0.019959  0.074474  0.019414  ...  0.010459  0.019003  0.056144  0.021896   \n",
      "\n",
      "       C506      C507      C508      C509      C510      C511  \n",
      "0  0.002297  0.028507  0.006407  0.030109  0.051867  0.082105  \n",
      "1  0.037209  0.036621  0.027617  0.042387  0.052971  0.016726  \n",
      "2  0.027177  0.036580  0.045591  0.028379  0.020328  0.034159  \n",
      "3  0.002088  0.022843  0.017706  0.061201  0.040269  0.007998  \n",
      "4  0.025158  0.043534  0.003123  0.096733  0.078068  0.071932  \n",
      "\n",
      "[5 rows x 512 columns]\n",
      "(400,)\n",
      "0    P002\n",
      "1    P000\n",
      "2    P000\n",
      "3    P001\n",
      "4    P001\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning\n",
    "prediction_var = list(test_df.columns)\n",
    "prediction_var.remove('target')\n",
    "\n",
    "# Get test data feature\n",
    "test_X = test_df[prediction_var]\n",
    "\n",
    "# Get test data target\n",
    "test_y = test_df.target\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_X.head())\n",
    "print(test_y.shape)\n",
    "print(test_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.svm.classes.SVC'> [0.         0.2027027  0.4047619  0.20535714]\n",
      "<class 'sklearn.ensemble.forest.ExtraTreesClassifier'> [0.93131868 0.33333333 0.42857143 0.15384615]\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> [0.93188011 0.23529412 0.33333333 0.16      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'> [0.92076503 0.         0.33333333 0.        ]\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'> [0.93837535 0.61538462 0.51612903 0.34482759]\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'P000':0.000001, 'P001': 29, 'P002': 14, 'P003':54}\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVC(gamma='scale', class_weight=my_dict,decision_function_shape = 'ovr', kernel='rbf'),\n",
    "    ExtraTreesClassifier(n_estimators=5, class_weight='balanced'),\n",
    "    RandomForestClassifier(n_estimators=5, class_weight='balanced'),\n",
    "    AdaBoostClassifier(n_estimators=5),\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "for model in classifiers:\n",
    "    clf = model\n",
    "\n",
    "    # Fit the model to training\n",
    "    clf.fit(train_X,train_y)\n",
    "\n",
    "    # Check prediction accuracy\n",
    "    prediction = clf.predict(test_X)\n",
    "    print(type(clf), metrics.f1_score(prediction,test_y,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on validation also, for the Kaggle submission\n",
    "\n",
    "s_train_df = pd.read_csv(s_trn_file_path, index_col='Unnamed: 0')\n",
    "s_train_df.drop(['name'], axis=1, inplace=True)\n",
    "\n",
    "s_test_df = pd.read_csv(s_val_file_path, index_col='Unnamed: 0')\n",
    "s_test_df.drop(['name'], axis=1, inplace=True )\n",
    "\n",
    "train_df = s_train_df\n",
    "train_df.target.fillna('P000', inplace=True)\n",
    "train_df.fillna(0.0, inplace=True)\n",
    "\n",
    "test_df = s_test_df\n",
    "test_df.target.fillna('P000', inplace=True)\n",
    "test_df.fillna(0.0, inplace=True)\n",
    "\n",
    "train_df = train_df.append(test_df, ignore_index=True)\n",
    "\n",
    "### tf_idf conversion\n",
    "\n",
    "# 1. Save target column, and drop if from dataframe\n",
    "train_df_target = pd.DataFrame(train_df['target'], columns=['target'])\n",
    "train_df.drop(['target'], axis=1, inplace=True )\n",
    "\n",
    "# 2. Replace frequencies with tf_idf scores\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(train_df)\n",
    "X_train_tf = tf_transformer.transform(train_df)\n",
    "train_df = pd.DataFrame(X_train_tf.todense(), columns=train_df.columns.values)\n",
    "\n",
    "# 3. Add back the target column\n",
    "train_df = pd.concat([train_df, train_df_target], axis=1)\n",
    "\n",
    "# Get input training data\n",
    "train_X = train_df[prediction_var]\n",
    "\n",
    "# Get input target variable\n",
    "train_y = train_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1699, 512)\n",
      "(1699,)\n"
     ]
    }
   ],
   "source": [
    "s_test_df = pd.read_csv(s_test_file_path, index_col='Unnamed: 0')\n",
    "\n",
    "name_list = s_test_df['name']\n",
    "\n",
    "s_test_df.drop(['name'], axis=1, inplace=True )\n",
    "\n",
    "test_df = s_test_df\n",
    "test_df.target.fillna('P000', inplace=True)\n",
    "test_df.fillna(0.0, inplace=True)\n",
    "\n",
    "### tf_idf conversion\n",
    "\n",
    "# 1. Save target column, and drop if from dataframe\n",
    "test_df_target = pd.DataFrame(test_df['target'], columns=['target'])\n",
    "test_df.drop(['target'], axis=1, inplace=True )\n",
    "\n",
    "# 2. Replace frequencies with tf_idf scores\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(test_df)\n",
    "X_train_tf = tf_transformer.transform(test_df)\n",
    "test_df = pd.DataFrame(X_train_tf.todense(), columns=test_df.columns.values)\n",
    "\n",
    "# 3. Add back the target column\n",
    "test_df = pd.concat([test_df, test_df_target], axis=1)\n",
    "\n",
    "# Machine Learning\n",
    "prediction_var = list(test_df.columns)\n",
    "prediction_var.remove('target')\n",
    "\n",
    "# Get test data features\n",
    "test_X = test_df[prediction_var]\n",
    "\n",
    "# Get test data target\n",
    "test_y = test_df.target\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "clf = svm.SVC(gamma='scale', class_weight=my_dict,decision_function_shape = 'ovr', probability=True)\n",
    "clf.fit(train_X,train_y)\n",
    "prediction = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prediction)):\n",
    "    if(prediction[i] == 'P001'):\n",
    "        prediction[i] = '1'\n",
    "    elif(prediction[i] == 'P002'):\n",
    "        prediction[i] = '2'\n",
    "    elif(prediction[i] == 'P003'):\n",
    "        prediction[i] = '3'\n",
    "    else:\n",
    "        prediction[i] = random.sample({1,2,3}, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\"VideoID\":name_list, \"Label\":prediction})\n",
    "output_df = output_df.set_index('VideoID')\n",
    "\n",
    "dict1 = output_df.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../all_test.video\", \"r\") as f:\n",
    "    video_list = f.readlines()\n",
    "    \n",
    "res = pd.DataFrame(columns=['VideoID', 'Label'])\n",
    "\n",
    "for line in video_list:\n",
    "    vid = line.strip(\"\\n\")\n",
    "    if(vid in dict1):\n",
    "        res = res.append({'VideoID': vid, 'Label':dict1[vid]['Label']}, ignore_index=True)\n",
    "    else:\n",
    "        res = res.append({'VideoID': vid, 'Label': random.sample({1,2,3}, 1)[0]}, ignore_index=True)\n",
    "\n",
    "res.to_csv(path_or_buf='../kaggle_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
